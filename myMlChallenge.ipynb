{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupiter Notebook you are going to train a classification model. Your aim would be to create a model which will give the highest score on the testset. The two
datasets which are imported into this Jupyter notebook are the training and testset. The testset contains the exact same features as the trainingset. The only difference is that the testset does not contain a column 'class'. Using a self trained classification model, you are going to predict the class of each entry in the testset. You will
train your classification model using the provided trainingset, which you will first need to wrangle.\n",
    "\n",
    "The datasets contain the following information:  \n",
    "- _id_: unique identifier  \n",
    "- _age_: continuous.  \n",
    "- _workclass_: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.  \n",
    "- _fnlwgt_: continuous.  \n",
    "- _education_: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.
 \n",
    "- _education-num_: continuous.  \n",
    "- _marital-status_: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.  \n",
    "- _occupation_: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.  \n",
    "- _relationship_: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.  \n",
    "- _race_: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.  \n",
    "- _sex_: Female, Male.  \n",
    "- _capital-gain_: continuous.  \n",
    "- _capital-loss_: continuous.  \n",
    "- _hours-per-week_: continuous.  \n",
    "- _native-country_: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.  \n",
    "- _class_: 0 = <=50k and 1 = >50k  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import\n",
    "The following reads the training and test set as a Panda DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_set = pd.read_csv('trainingset.csv')\n",
    "test_set = pd.read_csv('testset.csv')\n",
    "original_test_set = test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the data\n",
    "Start investigating the imported data. Use different methods to find out what the data looks like. Consult https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html for available functions on a Panda Dataframe. For example, find out which columns exist and what values it contains, check the number of entries, see if entries have missing values, group data, see if there is a correlation between fields. Knowing what your data looks like is a very important step before you can wrangle the data and start training a model.  \n",
    "A few handy functions:  \n",
    "- columns\n",
    "- ['age'] (select only the column age)\n",
    "- value_counts() (count unique values, ie train_set['age'].value_counts())\n",
    "- shape()\n",
    "- head()\n",
    "- mean(), sum(), max(), min(), unique()\n",
    "- loc[train_set['sex']=='Male'] (select all rows where sex == Male)\n",
    "- groupby(['race'])\n",
    "- describe (generate statistics about the data)\n",
    "\n",
    "Whenever you encounter the parameter _axis_, you can pass the values 0 or 1. 0 means a row and 1 means column of the dataframe\n",
    "\n",
    "Note that some methods can be used after other methods have been done to show more detailed information. For example you can first filter and on the filtered set, use a mean(). Or use a describe after you have grouped by  features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What features does the dataset contain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many persons earn a salary >50k and how many <=50k?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many men and women?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the number of Dutch persons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you display the average age per country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the education, marital status or workhours of persons with a salary higher than 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if you can come up with some interesting questions which you can answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle the data\n",
    "In this part you are going to prepare your data to be inserted into a learning algorithm. Think of removing empty entries, or filling in missing values, remove unwanted columns (ie id column, but __keep__ the class column, as it is needed in the next section), normalize the data. And most importantly, before you can train a model, make sure that each column only contains numbers. Most algorithms can only take numerics as input and not text. The most easiest way to do this is by assigning a number to every unique group of a feature.  \n",
    "Make sure to apply all wrangling on the training as well as on the testset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detect null values in the dataset. These functions might help with identifying null values and handling them:\n",
    "- isnull()\n",
    "- notnull()\n",
    "- dropna()\n",
    "- fillna()\n",
    "- replace()\n",
    "- interpolate()\n",
    "\n",
    "Remember to assign the result of a function to a variable, as a method does not directly edits the dataframe, but it returns a copy of the dataframe on which the method is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which columns have a null value, an example for the workclass is already given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['workclass'].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine yourself what you want to do with entries which a missing value. You can fill in missing values using an average, most common value, handle missing value as a new category, remove the entire feature, remove the entry, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the drop function on columns you plan on not using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that the features you are going to use have a number type and not a text type. This can be done using the factorize function. Note that there are two types
of text features, ordinal an nominal\n",
    "features.  \n",
    "An ordinal feature is a text feature with an ordening. For example education is ordinal as there is an ordening. Masters is a higher grade than Bachelors, which is higher than Some-college.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_education = ['Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th', 'HS-grad', 'Prof-school', 'Assoc-acdm', 'Assoc-voc', 'Some-college', 'Bachelors', 'Masters', 'Doctorate']\n",
    "cat_education = pd.Categorical(train_set.education, categories = ordered_education, ordered=True)\n",
    "labels, unique = pd.factorize(cat_education, sort=True)\n",
    "train_set.education = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that all ordinal features are transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nominal feature is a feature which does not have an ordening, for example the race feature does not have an ordening. If this type of feature is transformed in the
same way as ordinal features, \n",
    "the model we are going to train might assume that nearby values are similar, while all unique values are totally independent and different from each other.  \n",
    "For nominal features we can use a technique called 'One Hot Encoding' using the get_dummies function. This will create a new feature for each unique category with two possible values, 0 and 1.\n",
    "An example entry:  \n",
    "id, race  \n",
    "1, White  \n",
    "  \n",
    "Will become:  \n",
    "id, White, Black, Other  \n",
    "1, 1, 0, 0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_race = pd.Categorical(train_set['race'])\n",
    "dfDummies = pd.get_dummies(cat_race, prefix = 'race')\n",
    "dfDummies.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, concat the just created columns to your Panda DataFrame. Also, don't forget to drop the original column as it is not needed anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.concat([train_set, dfDummies], axis=1)\n",
    "train_set = train_set.drop('race', axis=1)\n",
    "train_set.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are able to transform all nominal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some additional wrangling on the data. For example, normalize data, remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting trainingset\n",
    "Before creating a model, it is handy to split the trainingset into two sets. One which is used to train your model, and the second to verify the quality of the trained model. A good split is 2/3 for training set and 1/3 for the test set. This way you will be able to verify if your trained model is any good, or that it needs to be altered.  \n",
    "After splitting, both the trainset and the testset will be splitted into a _x_ and _y_ vector, which will be used in the next section when the model is trained and evaluated. Make sure that the training and testing set contain the same (!) columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "split_train_set, split_test_set = train_test_split(train_set, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = split_train_set['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = split_train_set.drop(['class'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = split_test_set['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = split_test_set.drop(['class'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "When you are satisfied with the wrangled dataset, you can start to train your model. A model normally takes as input a set of features and a vector of classes belonging to the features.  \n",
    "If you are going to use Support Vector Machine, use sklearn.svm.SVC. Check out the documentation on which parameters exist and how to set them: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. Mind that the predictors of a SVM can never be categorical! Categorical variables must be converted to numbers or the algorithm will return an error. \n",
    " Example:\n",
    "\n",
    "    from sklearn.svm import SVC\n",
    "    svclassifier = SVC(kernel='linear')\n",
    "    svclassifier.fit(x, y)\n",
    "    y_pred = svclassifier.predict(x_test)\n",
    "\n",
    "If you are going to use Random Forest, use the sklearn.ensemble.RandomForestClassifier. Check out the documentation on which parameters exist and how to set them: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. \n",
    " Example:\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                  random_state=0)\n",
    "    clf.fit(x, y)\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "In both cases the _x _ is the array with features and _y _ is the vector of classes, which were created in the previous section. Method fit means your configured model is trained against a set of data, and predict means the classes of a given testset is predicted. The first time you create a model it is highly likely to generate an error of some sort. Usually a ValueError. Read the error attentively. These errors usually occur when data has not yet sufficiently been wrangled.\n",
    "\n",
    "Run the example below and examine the syntax. This is how a Random Forest and SVM are made. Mind that the SVM is quite slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_train_x = split_train_set[['age','education','hours-per-week']]\n",
    "example_test_x = x_test = split_test_set[['age','education','hours-per-week']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
    "clf.fit(example_train_x, y)\n",
    "y_pred_rf = clf.predict(example_test_x)\n",
    "accuracy_score(y_test, y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "svclassifier = SVC(kernel='linear')\n",
    "svclassifier.fit(example_train_x, y)\n",
    "y_pred_svm = svclassifier.predict(example_test_x)\n",
    "accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the trained model\n",
    "In this part you will test how well your model performs. This will be done against the split_test_set, so you can see how many entries you identified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict testset\n",
    "When you think the model is well trained, you can predict the classes of the testset using your trained model and export the results to a file. Make sure to put the results into the variable y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data export\n",
    "This part exports the data into __result.txt__. Make sure that y_pred is the prediction of the testset. The contents of the file will be in the form 'id,class' for each line in the file. This file can be uploaded on the challenge site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(original_test_set['id'])\n",
    "result['class'] = y_pred\n",
    "result.to_csv('result.txt', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}